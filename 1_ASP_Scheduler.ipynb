{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LLM_things import bots\n",
    "from ASP_Scheduler.problem_descriptions import all_problems\n",
    "from ASP_Scheduler import scheduler\n",
    "from ASP_Scheduler import schedulerBaseline\n",
    "import importlib\n",
    "from pprint import pprint\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We illustrate the generation of a scheduling problem using an Inference client, in this case deepseek (using chain-of-thought and few-shot prompting)\n",
    "# Notice you have to enter your api-key in line 132 of LLM_things/bots.py\n",
    "# Notice that delays of 10 seconds have been added in scheduler.py to avoid reaching the token-limit of the inference client.\n",
    "\n",
    "# Run the LLM scheduler\n",
    "full_program = scheduler.full_ASP_program(\n",
    "    all_problems['sports scheduling'],    # Input problem specifications for examination timetabling\n",
    "    pipe='deepseek',                                  # Input the PIPEline object for the LLM\n",
    "    printer=False)                              # Set to True to print intermediate outputs           \n",
    "\n",
    "# Print the full program as it is returned by the scheduler\n",
    "print('----------------------------FULL PROGRAM----------------------------')\n",
    "print(full_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We illustrate the generation of a scheduling problem using a local model  (using chain-of-thought and few-shot prompting)\n",
    "# Notice you have to have your api-key under .env\n",
    "\n",
    "\n",
    "# If we are working locally, we just instantiate to None and by default we will call the llama 3.1 8B api\n",
    "CHECKPOINT = None\n",
    "# CHECKPOINT = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "# If we work on a cloud, we can use other models like this\n",
    "PIPE = bots.load_pipe(model_checkpoint=CHECKPOINT, local_dir=\"./local_models\", quantization_config='4bit')\n",
    "\n",
    "# Examination Timetabling Example\n",
    "\n",
    "# Run the LLM scheduler\n",
    "full_program = scheduler.full_ASP_program(\n",
    "    all_problems['post_enrollment_based_course_time_tabling'],    # Input problem specifications for examination timetabling\n",
    "    pipe=PIPE,                                  # Input the PIPEline object for the LLM\n",
    "    printer=False)                              # Set to True to print intermediate outputs           \n",
    "\n",
    "# Print the full program as it is returned by the scheduler\n",
    "print('----------------------------FULL PROGRAM----------------------------')\n",
    "print(full_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We finally illustrate the generation of a scheduling problem using the baseline model.\n",
    "# PIPE has to be configured as above\n",
    "\n",
    "full_program = schedulerBaseline.full_ASP_program(\n",
    "    all_problems['nurse_scheduling'], \n",
    "    pipe=PIPE, \n",
    "    printer=False)\n",
    "\n",
    "print('----------------------------FULL PROGRAM----------------------------')\n",
    "print(full_program)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
